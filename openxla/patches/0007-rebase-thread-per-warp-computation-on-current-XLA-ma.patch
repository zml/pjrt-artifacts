From 50c10bb3208cc0edc22c0301ec0c9b5a83da0fae Mon Sep 17 00:00:00 2001
From: Hugo Mano <hugo@zml.ai>
Date: Tue, 27 May 2025 12:10:30 +0200
Subject: [PATCH 7/8] Rebase thread per warp computation on current XLA master

Only for ZML, no PR on XLA side.
---
 xla/backends/gpu/codegen/triton/compilation_pipeline.h | 1 +
 xla/pjrt/triton_cuda.cc                                | 8 +++++++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/xla/backends/gpu/codegen/triton/compilation_pipeline.h b/xla/backends/gpu/codegen/triton/compilation_pipeline.h
index 2bfa678adb..f7f550398f 100644
--- a/xla/backends/gpu/codegen/triton/compilation_pipeline.h
+++ b/xla/backends/gpu/codegen/triton/compilation_pipeline.h
@@ -21,6 +21,7 @@ limitations under the License.
 #include "absl/status/status.h"
 #include "mlir/Pass/PassManager.h"
 #include "xla/stream_executor/device_description.h"
+#include "xla/util.h"
 
 namespace mlir::triton::nvidia_gpu {
 
diff --git a/xla/pjrt/triton_cuda.cc b/xla/pjrt/triton_cuda.cc
index 963f4deade..5c36f11398 100644
--- a/xla/pjrt/triton_cuda.cc
+++ b/xla/pjrt/triton_cuda.cc
@@ -78,8 +78,14 @@ absl::Status TritonToLLVM(
     mlir::triton::nvidia_gpu::ClusterInfo* out_cluster_info) {
   mlir::PassManager pm(module.getContext());
   pm.enableVerifier();
+
+  stream_executor::DeviceDescription device_info;
+  TF_ASSIGN_OR_RETURN(stream_executor::GpuComputeCapability gpu_compute_capability,
+    stream_executor::CudaComputeCapability::FromString(arch_name));
+  device_info.set_gpu_compute_capability(gpu_compute_capability);
+
   TF_RETURN_IF_ERROR(
-      xla::gpu::CreateTritonPipeline(&pm, std::string(arch_name), num_warps,
+      xla::gpu::CreateTritonPipeline(&pm, device_info, num_warps,
                                      num_ctas, num_stages, *out_cluster_info,
                                      /*is_xla_fusion=*/false));
   return pm.run(module).succeeded()
-- 
2.39.5 (Apple Git-154)

