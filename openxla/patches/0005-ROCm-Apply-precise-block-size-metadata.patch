From 4c58e6b6965cfebaf45379e5c3554418109db0a5 Mon Sep 17 00:00:00 2001
From: Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>
Date: Tue, 11 Mar 2025 09:47:55 -0500
Subject: [PATCH 5/7] [ROCm] Apply precise block size metadata

---
 xla/backends/gpu/codegen/emitters/emitter_base.cc |  7 +++++++
 xla/backends/gpu/codegen/fusion_emitter.cc        | 14 ++++++++++++++
 xla/service/gpu/target_util.cc                    |  2 +-
 3 files changed, 22 insertions(+), 1 deletion(-)

diff --git a/xla/backends/gpu/codegen/emitters/emitter_base.cc b/xla/backends/gpu/codegen/emitters/emitter_base.cc
index 4d393924bf..41c351b045 100644
--- a/xla/backends/gpu/codegen/emitters/emitter_base.cc
+++ b/xla/backends/gpu/codegen/emitters/emitter_base.cc
@@ -36,6 +36,7 @@ limitations under the License.
 #include "llvm/IR/Function.h"
 #include "llvm/IR/IRBuilder.h"
 #include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/Linker/Linker.h"
 #include "llvm/Support/Casting.h"
@@ -135,26 +136,32 @@ void AddRanges(llvm::Function* func, const LaunchDimensions& launch_dims,
         if (auto* callee = call->getCalledFunction()) {
           switch (callee->getIntrinsicID()) {
             case llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x:
+            case llvm::Intrinsic::amdgcn_workitem_id_x:
               llvm_ir::AddRangeMetadata(
                   0, launch_dims.thread_counts_per_block().x, call, module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y:
+            case llvm::Intrinsic::amdgcn_workitem_id_y:
               llvm_ir::AddRangeMetadata(
                   0, launch_dims.thread_counts_per_block().y, call, module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z:
+            case llvm::Intrinsic::amdgcn_workitem_id_z:
               llvm_ir::AddRangeMetadata(
                   0, launch_dims.thread_counts_per_block().z, call, module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x:
+            case llvm::Intrinsic::amdgcn_workgroup_id_x:
               llvm_ir::AddRangeMetadata(0, launch_dims.block_counts().x, call,
                                         module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y:
+            case llvm::Intrinsic::amdgcn_workgroup_id_y:
               llvm_ir::AddRangeMetadata(0, launch_dims.block_counts().y, call,
                                         module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z:
+            case llvm::Intrinsic::amdgcn_workgroup_id_z:
               llvm_ir::AddRangeMetadata(0, launch_dims.block_counts().z, call,
                                         module);
               break;
diff --git a/xla/backends/gpu/codegen/fusion_emitter.cc b/xla/backends/gpu/codegen/fusion_emitter.cc
index 9c02768ddf..419a940805 100644
--- a/xla/backends/gpu/codegen/fusion_emitter.cc
+++ b/xla/backends/gpu/codegen/fusion_emitter.cc
@@ -83,6 +83,9 @@ absl::Status AnnotateKernelLaunchDimensions(
   // Add __launch_bounds__ to metadata. This limits registers per thread to
   // avoid out-of-resources launching errors.
 
+  llvm::Triple target_triple = llvm::Triple(llvm_module->getTargetTriple());
+
+  if (target_triple.isNVPTX()) {
   // Our launch bounds are exact, so we can specify them as
   // reqntid[xyz] rather than maxntid[xyz].
   const std::string attr =
@@ -92,6 +95,17 @@ absl::Status AnnotateKernelLaunchDimensions(
   kernel->addFnAttr("nvvm.reqntid", attr);
   // Maybe we want to set "reqnctapercluster" here, but not sure if needed or if
   // LLVM supports that yet. Let's do that later when needed.
+  } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
+    kernel->addFnAttr("amdgpu-flat-work-group-size",
+                         absl::StrJoin({launch_dims.num_threads_per_block(),
+                                        launch_dims.num_threads_per_block()},
+                                       ","));
+    kernel->addFnAttr("amdgpu-max-num-workgroups",
+                         absl::StrJoin({launch_dims.block_counts().x,
+                                        launch_dims.block_counts().y,
+                                        launch_dims.block_counts().z},
+                                       ","));
+  }
   return absl::OkStatus();
 }
 
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 9c829183ab..8128f4022a 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -468,7 +468,7 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,
   } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.
     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);
-    func->addFnAttr("amdgpu-flat-work-group-size", "1, 1024");
+    func->addFnAttr("uniform-work-group-size", "true");
   } else if (target_triple.isSPIR()) {
     // Attach information so that it can be recognized as a SPIR kernel.
     func->setCallingConv(llvm::CallingConv::SPIR_KERNEL);
-- 
2.39.5 (Apple Git-154)

