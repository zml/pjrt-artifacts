From 63439cf2d4d9499d5edc8460c197969e4673305e Mon Sep 17 00:00:00 2001
From: Hugo Mano <hugo@zml.ai>
Date: Tue, 27 May 2025 11:53:49 +0200
Subject: [PATCH 4/5] [ROCm] Triton performance fixes

PR: https://github.com/openxla/xla/pull/23688
---
 .../gpu/codegen/emitters/emitter_base.cc        |  7 +++++++
 xla/backends/gpu/codegen/fusion_emitter.cc      | 14 ++++++++++++++
 .../gpu/codegen/triton/compilation_pipeline.h   |  8 ++++++--
 .../codegen/triton/compilation_pipeline_cuda.cc |  6 ++----
 .../codegen/triton/compilation_pipeline_rocm.cc | 17 ++++++++---------
 .../codegen/triton/compilation_pipeline_stub.cc |  6 ++++--
 .../gpu/codegen/triton/fusion_emitter.cc        |  4 +---
 .../codegen/triton/fusion_emitter_stub_test.cc  |  2 +-
 xla/pjrt/triton_cuda.cc                         |  8 +++++++-
 xla/service/gpu/ir_emitter_unnested.cc          |  7 ++++---
 xla/service/gpu/target_util.cc                  |  2 +-
 11 files changed, 55 insertions(+), 26 deletions(-)

diff --git a/xla/backends/gpu/codegen/emitters/emitter_base.cc b/xla/backends/gpu/codegen/emitters/emitter_base.cc
index 7555d06378..db6ffaf77c 100644
--- a/xla/backends/gpu/codegen/emitters/emitter_base.cc
+++ b/xla/backends/gpu/codegen/emitters/emitter_base.cc
@@ -37,6 +37,7 @@ limitations under the License.
 #include "llvm/IR/Function.h"
 #include "llvm/IR/IRBuilder.h"
 #include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/Linker/Linker.h"
 #include "llvm/Support/Casting.h"
@@ -137,26 +138,32 @@ void AddRanges(llvm::Function* func, const LaunchDimensions& launch_dims,
         if (auto* callee = call->getCalledFunction()) {
           switch (callee->getIntrinsicID()) {
             case llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x:
+            case llvm::Intrinsic::amdgcn_workitem_id_x:
               llvm_ir::AddRangeMetadata(
                   0, launch_dims.thread_counts_per_block().x, call, module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y:
+            case llvm::Intrinsic::amdgcn_workitem_id_y:
               llvm_ir::AddRangeMetadata(
                   0, launch_dims.thread_counts_per_block().y, call, module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z:
+            case llvm::Intrinsic::amdgcn_workitem_id_z:
               llvm_ir::AddRangeMetadata(
                   0, launch_dims.thread_counts_per_block().z, call, module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x:
+            case llvm::Intrinsic::amdgcn_workgroup_id_x:
               llvm_ir::AddRangeMetadata(0, launch_dims.block_counts().x, call,
                                         module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y:
+            case llvm::Intrinsic::amdgcn_workgroup_id_y:
               llvm_ir::AddRangeMetadata(0, launch_dims.block_counts().y, call,
                                         module);
               break;
             case llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z:
+            case llvm::Intrinsic::amdgcn_workgroup_id_z:
               llvm_ir::AddRangeMetadata(0, launch_dims.block_counts().z, call,
                                         module);
               break;
diff --git a/xla/backends/gpu/codegen/fusion_emitter.cc b/xla/backends/gpu/codegen/fusion_emitter.cc
index 86f5eae326..57ce7440a9 100644
--- a/xla/backends/gpu/codegen/fusion_emitter.cc
+++ b/xla/backends/gpu/codegen/fusion_emitter.cc
@@ -79,6 +79,9 @@ absl::Status AnnotateKernelLaunchDimensions(
   // Add __launch_bounds__ to metadata. This limits registers per thread to
   // avoid out-of-resources launching errors.
 
+  llvm::Triple target_triple = llvm::Triple(llvm_module->getTargetTriple());
+
+  if (target_triple.isNVPTX()) {
   // Our launch bounds are exact, so we can specify them as
   // reqntid[xyz] rather than maxntid[xyz].
   const std::string attr =
@@ -88,6 +91,17 @@ absl::Status AnnotateKernelLaunchDimensions(
   kernel->addFnAttr("nvvm.reqntid", attr);
   // Maybe we want to set "reqnctapercluster" here, but not sure if needed or if
   // LLVM supports that yet. Let's do that later when needed.
+  } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
+    kernel->addFnAttr("amdgpu-flat-work-group-size",
+                         absl::StrJoin({launch_dims.num_threads_per_block(),
+                                        launch_dims.num_threads_per_block()},
+                                       ","));
+    kernel->addFnAttr("amdgpu-max-num-workgroups",
+                         absl::StrJoin({launch_dims.block_counts().x,
+                                        launch_dims.block_counts().y,
+                                        launch_dims.block_counts().z},
+                                       ","));
+  }
   return absl::OkStatus();
 }
 
diff --git a/xla/backends/gpu/codegen/triton/compilation_pipeline.h b/xla/backends/gpu/codegen/triton/compilation_pipeline.h
index 9acd6fee99..f7f550398f 100644
--- a/xla/backends/gpu/codegen/triton/compilation_pipeline.h
+++ b/xla/backends/gpu/codegen/triton/compilation_pipeline.h
@@ -20,6 +20,8 @@ limitations under the License.
 
 #include "absl/status/status.h"
 #include "mlir/Pass/PassManager.h"
+#include "xla/stream_executor/device_description.h"
+#include "xla/util.h"
 
 namespace mlir::triton::nvidia_gpu {
 
@@ -41,8 +43,10 @@ namespace gpu {
 // parameter which would give a hint to Triton which cluster dims we prefer to
 // use, but that's not the case currently.
 absl::Status CreateTritonPipeline(
-    mlir::OpPassManager* pm, std::string arch_name, int num_warps, int num_ctas,
-    int num_stages, mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info,
+    mlir::OpPassManager* pm,
+    const stream_executor::DeviceDescription& device_info, int num_warps,
+    int num_ctas, int num_stages,
+    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info,
     bool is_xla_fusion);
 
 }  // namespace gpu
diff --git a/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc b/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc
index ad57c6d982..f3ae864b7b 100644
--- a/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc
+++ b/xla/backends/gpu/codegen/triton/compilation_pipeline_cuda.cc
@@ -42,13 +42,11 @@ namespace mt = ::mlir::triton;
 namespace mt_xla = ::mlir::triton::xla;
 
 absl::Status CreateTritonPipeline(mlir::OpPassManager* pm,
-                                  std::string arch_name, int num_warps,
+                                  const se::DeviceDescription& device_info, int num_warps,
                                   int num_ctas, int num_stages,
                                   mt::nvidia_gpu::ClusterInfo& out_cluster_info,
                                   bool is_xla_fusion) {
-  TF_ASSIGN_OR_RETURN(
-      const stream_executor::CudaComputeCapability cc,
-      stream_executor::CudaComputeCapability::FromString(arch_name));
+  auto cc = device_info.cuda_compute_capability();
   const int ccAsInt = cc.major * 10 + cc.minor;
   const int threadsPerWarp = 32;
 
diff --git a/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc b/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc
index dc22968a46..68bc164538 100644
--- a/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc
+++ b/xla/backends/gpu/codegen/triton/compilation_pipeline_rocm.cc
@@ -53,14 +53,13 @@ using ::mlir::Type;
 using ::mlir::Value;
 using mlir::ValueRange;
 
-absl::Status CreateTritonPipeline(mlir::OpPassManager* pm,
-                                  std::string arch_name, int num_warps,
-                                  int num_ctas, int num_stages,
-                                  mt::nvidia_gpu::ClusterInfo& out_cluster_info,
-                                  bool is_xla_fusion) {
-  // TODO(ROCm): Check why some test fail when threadsPerWarp is set to 64.
-  const int threadsPerWarp = 32;
-  auto cc = se::RocmComputeCapability(std::move(arch_name));
+absl::Status CreateTritonPipeline(
+    mlir::OpPassManager* pm,
+    const stream_executor::DeviceDescription& device_info, int num_warps,
+    int num_ctas, int num_stages, mt::nvidia_gpu::ClusterInfo& out_cluster_info,
+    bool is_xla_fusion) {
+  const int threadsPerWarp = device_info.threads_per_warp();
+  auto cc = device_info.rocm_compute_capability();
 
   if (is_xla_fusion) {
     pm->addPass(mt_xla::CreateInt4ToPackedInt4RewritePass());
@@ -128,7 +127,7 @@ absl::Status CreateTritonPipeline(mlir::OpPassManager* pm,
   if (/*use_buffer_ops=*/false) {  // Not enabled by default.
     pm->addPass(mlir::createTritonAMDGPUCanonicalizePointersPass());
     pm->addPass(mlir::createCanonicalizerPass());
-    pm->addPass(mlir::createTritonAMDGPUConvertToBufferOpsPass(arch_name));
+    pm->addPass(mlir::createTritonAMDGPUConvertToBufferOpsPass(cc.gfx_version()));
   }
   pm->addPass(mlir::createTritonAMDGPUFoldTrueCmpIPass());
   pm->addPass(mlir::createCanonicalizerPass());
diff --git a/xla/backends/gpu/codegen/triton/compilation_pipeline_stub.cc b/xla/backends/gpu/codegen/triton/compilation_pipeline_stub.cc
index d91acda7f5..4b9d7f0949 100644
--- a/xla/backends/gpu/codegen/triton/compilation_pipeline_stub.cc
+++ b/xla/backends/gpu/codegen/triton/compilation_pipeline_stub.cc
@@ -23,8 +23,10 @@ namespace xla {
 namespace gpu {
 
 absl::Status CreateTritonPipeline(
-    mlir::OpPassManager* pm, std::string arch_name, int num_warps, int num_ctas,
-    int num_stages, mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info,
+    mlir::OpPassManager* pm,
+    const stream_executor::DeviceDescription& device_info, int num_warps,
+    int num_ctas, int num_stages,
+    mlir::triton::nvidia_gpu::ClusterInfo& out_cluster_info,
     bool is_xla_fusion) {
   return absl::UnimplementedError("not supported for this build configuration");
 }
diff --git a/xla/backends/gpu/codegen/triton/fusion_emitter.cc b/xla/backends/gpu/codegen/triton/fusion_emitter.cc
index 1bfe1aac63..62c83cd90a 100644
--- a/xla/backends/gpu/codegen/triton/fusion_emitter.cc
+++ b/xla/backends/gpu/codegen/triton/fusion_emitter.cc
@@ -1810,8 +1810,6 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(
     mlir::ModuleOp triton_module, llvm::Module* llvm_module,
     mlir::MLIRContext& mlir_context, bool is_xla_fusion, bool emit_kernel) {
   const auto& cc = device_info.gpu_compute_capability();
-  std::string arch_name =
-      std::visit([](auto& cc) { return cc.ToString(); }, cc);
   if (std::holds_alternative<se::CudaComputeCapability>(cc)) {
     auto ccCuda = std::get<se::CudaComputeCapability>(cc);
     if (!ccCuda.IsAtLeastAmpere()) {
@@ -1895,7 +1893,7 @@ absl::StatusOr<TritonWrapperResult> CompileTritonToLLVM(
   }
 
   mlir::triton::nvidia_gpu::ClusterInfo cluster_info;
-  if (!CreateTritonPipeline(&pm, arch_name, num_warps, num_ctas, num_stages,
+  if (!CreateTritonPipeline(&pm, device_info, num_warps, num_ctas, num_stages,
                             cluster_info, is_xla_fusion)
            .ok()) {
     return Internal("Failed to create Triton pipeline.");
diff --git a/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc b/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc
index 9c7e30dd78..c3e59bdeb3 100644
--- a/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc
+++ b/xla/backends/gpu/codegen/triton/fusion_emitter_stub_test.cc
@@ -53,7 +53,7 @@ TEST(TritonStub, CallStubApi) {
   mlir::OpPassManager pm;
   ::mlir::triton::nvidia_gpu::ClusterInfo cluster_info;
 
-  EXPECT_FALSE(CreateTritonPipeline(&pm, "", 1, 1, 1, cluster_info,
+  EXPECT_FALSE(CreateTritonPipeline(&pm, {}, 1, 1, 1, cluster_info,
                                     /*is_xla_fusion=*/true)
                    .ok());
   EXPECT_EQ(GetLibdevicePath({}, {}), "");
diff --git a/xla/pjrt/triton_cuda.cc b/xla/pjrt/triton_cuda.cc
index 963f4deade..5c36f11398 100644
--- a/xla/pjrt/triton_cuda.cc
+++ b/xla/pjrt/triton_cuda.cc
@@ -78,8 +78,14 @@ absl::Status TritonToLLVM(
     mlir::triton::nvidia_gpu::ClusterInfo* out_cluster_info) {
   mlir::PassManager pm(module.getContext());
   pm.enableVerifier();
+
+  stream_executor::DeviceDescription device_info;
+  TF_ASSIGN_OR_RETURN(stream_executor::GpuComputeCapability gpu_compute_capability,
+    stream_executor::CudaComputeCapability::FromString(arch_name));
+  device_info.set_gpu_compute_capability(gpu_compute_capability);
+
   TF_RETURN_IF_ERROR(
-      xla::gpu::CreateTritonPipeline(&pm, std::string(arch_name), num_warps,
+      xla::gpu::CreateTritonPipeline(&pm, device_info, num_warps,
                                      num_ctas, num_stages, *out_cluster_info,
                                      /*is_xla_fusion=*/false));
   return pm.run(module).succeeded()
diff --git a/xla/service/gpu/ir_emitter_unnested.cc b/xla/service/gpu/ir_emitter_unnested.cc
index 1339eb9312..9e7cb5573e 100644
--- a/xla/service/gpu/ir_emitter_unnested.cc
+++ b/xla/service/gpu/ir_emitter_unnested.cc
@@ -1445,9 +1445,10 @@ absl::Status IrEmitterUnnested::EmitTritonCustomCall(
             ir_emitter_context_->buffer_assignment(),
             GetDefaultBufferAlignment(), instr, instr->operands(),
             /*dedup=*/false));
-    auto launch_dimensions =
-        LaunchDimensions(se::BlockDim(call.grid_x, call.grid_y, call.grid_z),
-                         se::ThreadDim(call.num_warps * 32));
+    auto launch_dimensions = LaunchDimensions(
+        se::BlockDim(call.grid_x, call.grid_y, call.grid_z),
+        se::ThreadDim(call.num_warps *
+                      ir_emitter_context_->gpu_device_info().threads_per_warp()));
 
     std::string sanitized_kernel_name =
         GetSanitizedUniqueName(*ir_emitter_context_, kernel_name);
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 89b11b4550..17ef9eb3d0 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -472,7 +472,7 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,
   } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.
     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);
-    func->addFnAttr("amdgpu-flat-work-group-size", "1, 1024");
+    func->addFnAttr("uniform-work-group-size", "true");
   } else if (target_triple.isSPIR()) {
     // Attach information so that it can be recognized as a SPIR kernel.
     func->setCallingConv(llvm::CallingConv::SPIR_KERNEL);
-- 
2.39.5 (Apple Git-154)

